{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import struct\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    # default hyperparameters for the Llama 7B model\n",
    "    dim: int = 4096  # 模型维度\n",
    "    n_layers: int = 32  # Transformer层数\n",
    "    n_heads: int = 32  # 注意力机制的头数\n",
    "    n_kv_heads: Optional[int] = None  # 键/值头数，如果未指定，则默认为n_heads\n",
    "    vocab_size: int = 32000  # 词汇表大小\n",
    "    hidden_dim: Optional[int] = None  # 隐藏层维度，如果未指定，则使用其他规则确定\n",
    "    multiple_of: int = 256  # MLP隐藏层大小是这个数的倍数\n",
    "    norm_eps: float = 1e-5  # 归一化层的epsilon值\n",
    "    max_seq_len: int = 2048  # 最大序列长度\n",
    "    dropout: float = 0.0  # 丢弃率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ModelArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama2的RMSNorm层的公式如下：\n",
    "\n",
    "$$\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}w_i^2 + \\epsilon}}$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- ( $x$ ) 是层的输入。\n",
    "- ( $w_i$ ) 代表层的权重。\n",
    "- ( $n$ ) 是权重的数量。\n",
    "- ( $\\epsilon$ ) 是一个小常数，用于数值稳定性（以避免除以零的情况）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = RMSNorm(dim=args.dim, eps=args.norm_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 4096])\n",
      "torch.Size([1, 50, 4096])\n"
     ]
    }
   ],
   "source": [
    "# 写一个关于norm的测试\n",
    "\n",
    "x = torch.randn(1, 50, 4096) # bs, seq_len, dim\n",
    "print(x.shape)\n",
    "print(norm(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设 $\\text{dim}$ 是输入维度，$\\text{end}$ 是序列的长度，$\\theta$ 是比例因子（默认为 10000.0）。\n",
    "\n",
    "1. **频率计算**:\n",
    "   $$\\text{freqs} = \\frac{1}{\\theta^{\\frac{2i}{\\text{dim}}}}$$\n",
    "   其中 $i = 0, 1, 2, ..., \\frac{\\text{dim}}{2} - 1$。\n",
    "\n",
    "2. **时间序列与频率的外积**:\n",
    "   创建一个从 0 到 $\\text{end} - 1$ 的时间序列$t$，并计算 $t$ 和 $\\text{freqs}$ 的外积得到频率矩阵。\n",
    "\n",
    "3. **余弦和正弦值计算**:\n",
    "   - 余弦值：$\\text{freqs\\_cos} = \\cos(\\text{freqs\\_matrix})$\n",
    "   - 正弦值：$\\text{freqs\\_sin} = \\sin(\\text{freqs\\_matrix})$\n",
    "\n",
    "其中，$\\text{freqs\\_matrix}$ 是时间序列 $t$ 和频率$ \\text{freqs}$ 的外积的结果。\n",
    "\n",
    "\n",
    "这个例子首先定义了函数 `precompute_freqs_cis`。然后，它设置了维度 `dim` 为 10，序列长度 `end` 为 5，并保持默认的比例因子$\\theta = 10000.0$。通过调用这个函数并传入这些参数，它计算了序列中每个位置的余弦和正弦值。最后，这个例子打印了这些计算得到的余弦和正弦值矩阵。\n",
    "\n",
    "这种预计算的余弦和正弦值可以用于例如 Transformer 模型中的位置编码，以提供位置信息，帮助模型理解输入数据中元素的顺序关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cos = torch.cos(freqs)  # real part\n",
    "    freqs_sin = torch.sin(freqs)  # imaginary part\n",
    "    return freqs_cos, freqs_sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2048]), torch.Size([50]), torch.Size([50, 2048]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta=10000.0\n",
    "dim=4096\n",
    "end=50\n",
    "\n",
    "freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "t = torch.arange(end, device=freqs.device)\n",
    "res = torch.outer(t, freqs).float() \n",
    "freqs.shape, t.shape, res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 2048]), torch.Size([50, 2048]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos, sin = precompute_freqs_cis(4096, 50)\n",
    "cos.shape, sin.shape    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reshape_for_broadcast 函数\n",
    "\n",
    "主要作用： 该函数的目的是为了将频率的余弦（cos）和正弦（sin）张量重新塑形（reshape），使其能够在后续的旋转操作中通过广播机制与查询（query）或键（key）张量进行元素级的乘法操作。广播是一种在不同形状的张量之间进行数学运算的方式，能够自动扩展张量的形状以匹配操作的需求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim  # 获取x的维度数量\n",
    "    assert 0 <= 1 < ndim  # 确保x至少有两个维度\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])  # 确保频率张量的形状与x的第二个维度和最后一个维度匹配\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]  # 生成一个新的形状，除了第二个和最后一个维度，其他维度设置为1\n",
    "    return freqs_cis.view(shape)  # 返回重新塑形的频率张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply_rotary_emb 函数\n",
    "\n",
    "主要作用： 该函数实现了旋转位置编码的应用过程。它首先将查询（query）和键（key）张量转换为复数形式（这里使用实部和虚部的形式分别表示），然后利用传入的余弦和正弦频率张量对它们进行旋转，最后将旋转后的结果转换回原来的形状。这个过程可以增强模型对每个位置信息的感知能力，从而提高处理序列数据的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cos: torch.Tensor,\n",
    "    freqs_sin: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    # 将查询和键张量转换为浮点数，并重塑形状以分离实部和虚部\n",
    "    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "\n",
    "    # 重新塑形频率张量以进行广播\n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)\n",
    "\n",
    "    # 应用旋转，分别计算旋转后的实部和虚部\n",
    "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "\n",
    "    # 将最后两个维度合并，并还原为原始张量的形状\n",
    "    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(3)\n",
    "    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(3)\n",
    "\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 144]) torch.Size([50, 144])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 50, 144, 2]), torch.Size([1, 50, 144, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 根据上述代码 为apply_rotary_emb函数写一个测试案例\n",
    "\n",
    "xq = torch.randn(1, 50, 288) # bs, seq_len, dim\n",
    "xk = torch.randn(1, 50, 288) # bs, seq_len, dim\n",
    "\n",
    "# 使用 precompute_freqs_cis 函数获取 sin和cos\n",
    "\n",
    "cos, sin = precompute_freqs_cis(288, 50)\n",
    "print(cos.shape, sin.shape)\n",
    "xq_out, xk_out = apply_rotary_emb(xq, xk, cos, sin)\n",
    "\n",
    "xq_out.shape, xk_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repeat_kv\n",
    "\n",
    "根据 n_rep 参数的值重复每个键（key）和值（value）元素。如果 n_rep 为1，表示不需要重复，直接返回原始张量。如果 n_rep 大于1，函数会将输入张量 x 在特定的维度上重复 n_rep 次，然后重新组织张量的形状以适应重复后的结构。\n",
    "\n",
    "输入张量 x 在键/值维度 (n_kv_heads) 上被重复了 n_rep 次，且这种重复是在不改变其他维度（如批量大小、序列长度、头的维度）的情况下实现的。这使得在 Transformer 模型中可以灵活地调整键和值的数量，以适应不同的模型架构或实验设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    # 获取输入张量的形状：批量大小、序列长度、键/值对头的数量、每个头的维度大小\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    \n",
    "    # 如果重复次数为1，则不需要重复，直接返回原始张量\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    \n",
    "    # 对张量进行扩展和重塑操作以重复键值对\n",
    "    return (\n",
    "        x[:, :, :, None, :]  # 在第四个维度（头的维度前）添加一个新的维度\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)  # 将新添加的维度扩展到n_rep大小，实现重复的效果\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)  # 重新塑形，合并键/值对头的数量和重复次数的维度\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        # 根据是否指定n_kv_heads，确定用于键（key）和值（value）的头的数量。\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        # 确保总头数可以被键值头数整除。\n",
    "        assert args.n_heads % self.n_kv_heads == 0\n",
    "\n",
    "        # 模型并行处理大小，默认为1。\n",
    "        model_parallel_size = 1\n",
    "        # 本地计算头数，等于总头数除以模型并行处理大小。\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        # 本地键值头数，等于键值头数除以模型并行处理大小。\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
    "        # 重复次数，用于扩展键和值的尺寸。\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        # 每个头的维度，等于模型维度除以头的总数。\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        # 定义权重矩阵。\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        # 输出权重矩阵。\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "        # 定义dropout。\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        self.resid_dropout = nn.Dropout(args.dropout)\n",
    "        # 保存dropout概率。\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        # 检查是否使用Flash Attention（需要PyTorch >= 2.0）。\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            # 若不支持Flash Attention，则使用手动实现的注意力机制，并设置mask。\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # 创建一个上三角矩阵，用于遮蔽未来信息。\n",
    "            mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float(\"-inf\"))\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            # 注册为模型的缓冲区\n",
    "            self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        # 获取批次大小和序列长度。\n",
    "        bsz, seqlen, _ = x.shape\n",
    "\n",
    "        # 计算查询（Q）、键（K）、值（V）。\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "        # 调整形状以适应头的维度。\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "\n",
    "        # 应用旋转位置嵌入（RoPE）。\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)\n",
    "\n",
    "        # 对键和值进行扩展以适应重复次数。\n",
    "        xk = repeat_kv(xk, self.n_rep)\n",
    "        xv = repeat_kv(xv, self.n_rep)\n",
    "\n",
    "        # 将头作为批次维度处理。\n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "\n",
    "        # 根据是否支持Flash Attention，选择实现方式。\n",
    "        if self.flash:\n",
    "            # 使用Flash Attention。\n",
    "            output = torch.nn.functional.scaled_dot_product_attention(xq, xk, xv, attn_mask=None, dropout_p=self.dropout if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            # 使用手动实现的注意力机制。\n",
    "            scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "            assert hasattr(self, 'mask')\n",
    "            scores = scores + self.mask[:, :, :seqlen, :seqlen]\n",
    "            scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "            scores = self.attn_dropout(scores)\n",
    "            output = torch.matmul(scores, xv)\n",
    "\n",
    "        # 恢复时间维度并合并头。\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "\n",
    "        # 最终投影回残差流。\n",
    "        output = self.wo(output)\n",
    "        output = self.resid_dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [-inf, -inf, -inf, -inf, -inf, -inf]]]])\n",
      "tensor([[[[0., -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.full((1, 1, 6, 6), float(\"-inf\"))\n",
    "print(mask)\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 24]) torch.Size([50, 24])\n",
      "Output shape: torch.Size([2, 50, 288])\n"
     ]
    }
   ],
   "source": [
    "class ModelArgs:\n",
    "    def __init__(self, dim, n_heads, n_kv_heads, max_seq_len, dropout):\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "args = ModelArgs(\n",
    "    dim=288, \n",
    "    n_heads=6, \n",
    "    n_kv_heads=6, \n",
    "    max_seq_len=100,  # 假设序列的最大长度为100\n",
    "    dropout=0.0\n",
    ")\n",
    "\n",
    "# 创建Attention实例\n",
    "attention_model = Attention(args)\n",
    "\n",
    "# 模拟输入数据\n",
    "batch_size = 2\n",
    "seq_len = 50  # 假设实际使用的序列长度为50\n",
    "dim = args.dim\n",
    "x = torch.rand(batch_size, seq_len, dim)  # 随机生成输入张量\n",
    "# freqs_cos = torch.rand(seq_len, dim // 2)  # 模拟cos频率，用于RoPE\n",
    "# freqs_sin = torch.rand(seq_len, dim // 2)  # 模拟sin频率，用于RoPE\n",
    "\n",
    "freqs_cos, freqs_sin = precompute_freqs_cis(dim//args.n_heads, seq_len)\n",
    "\n",
    "print(freqs_cos.shape, freqs_sin.shape)\n",
    "\n",
    "# 运行Attention模型\n",
    "output = attention_model(x, freqs_cos, freqs_sin)\n",
    "\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 288])\n",
      "Assertion error: \n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(batch_size, seq_len, dim)  # 假设输入张量\n",
    "print(x.shape)\n",
    "try:\n",
    "    freqs_cos, freqs_sin = precompute_freqs_cis(dim=dim//args.n_heads, end=seq_len)\n",
    "    freqs_cos_b = reshape_for_broadcast(freqs_cos, x)\n",
    "    freqs_sin_b = reshape_for_broadcast(freqs_sin, x)\n",
    "    print(\"Broadcast shapes:\", freqs_cos_b.shape, freqs_sin_b.shape)\n",
    "except AssertionError as e:\n",
    "    print(\"Assertion error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
